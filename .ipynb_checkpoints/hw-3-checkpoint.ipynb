{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv \n",
    "import re\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"homework.txt\", \"w\")\n",
    "\n",
    "for i in range(1,301):\n",
    "    page = requests.get(\"https://www.goodreads.com/list/show/1.Best_Books_Ever?page=\" + str(i))\n",
    "    soup = BeautifulSoup(page.content, features=\"lxml\")\n",
    "    links = soup.find_all('a', itemprop='url', class_='bookTitle')\n",
    "    for link in links:\n",
    "        fullLink = link.get('href')\n",
    "        f.write('https://www.goodreads.com' + fullLink + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './storage/'\n",
    "\n",
    "\n",
    "for i in range(1, 29701):\n",
    "    \n",
    "    folderName = \"folder-\" + str(i) + \"/\"\n",
    "    fileName = \"article_\" + str(i) + \".html\"\n",
    "    \n",
    "    f = open(\"homework3.txt\", \"r\")\n",
    "    url = f.readlines()[i-1]\n",
    "    \n",
    "    Path(path + folderName).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    page = requests.get(url)\n",
    "    code = str(page.text)\n",
    "\n",
    "    with open(path + folderName + fileName, \"w\", encoding=\"utf-8\") as z:\n",
    "        z.write(code)\n",
    "\n",
    "    z.close()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This book is not in english: 9932\n"
     ]
    }
   ],
   "source": [
    "data = ['bookTitle', 'bookSeries', 'bookAuthors', 'ratingValue', \"ratingCount\",\\\n",
    "        \"reviewCount\", \"plot\", \"numberOfPages\", \"publishingDate\", \"characters\", \"setting\", \"url\"]\n",
    "\n",
    "personalPath = \"C:/Users/giogi/Data Science/Algorithmic Methods of Data Mining/storage/folder-\"\n",
    "\n",
    "for i in range(9901, 9934): #9901\n",
    "    with open(personalPath + str(i) + \"/article_\" + str(i) + \".html\", 'rb') as html: \n",
    "        soup = BeautifulSoup(html,\"html.parser\")\n",
    "    #plot = soup.find('div',id='description').text.strip()  \n",
    "    try:\n",
    "        plot = soup.find('div',id='description').text.strip()\n",
    "        if detect(plot)=='en':\n",
    "            lista=[]\n",
    "\n",
    "            #title\n",
    "            try:\n",
    "                lista.append(soup.find('h1').text.strip())\n",
    "            except:\n",
    "                lista.append('')\n",
    "\n",
    "                #bookseries\n",
    "            try:\n",
    "                lista.append(soup.find('h2',id='bookSeries').text.strip())\n",
    "            except:\n",
    "                lista.append('')\n",
    "\n",
    "                #author name\n",
    "            try:\n",
    "                lista.append(soup.find('a',class_='authorName').text.strip())\n",
    "            except:\n",
    "                lista.append('')\n",
    "\n",
    "                #rating value\n",
    "            try:\n",
    "                lista.append(soup.find('span', itemprop='ratingValue').text.strip())\n",
    "            except:\n",
    "                lista.append('')\n",
    "\n",
    "                #ratingCount\n",
    "            try:\n",
    "                lista.append(soup.find_all('a',class_='gr-hyperlink',href='#other_reviews')[0]\\\n",
    "                             .text.strip().replace('\\r', '').replace('\\n', '').split()[0])\n",
    "            except:\n",
    "                lista.append('')\n",
    "\n",
    "                #reviewCount\n",
    "            try:\n",
    "                lista.append(soup.find_all('a',class_='gr-hyperlink',href='#other_reviews')[1]\\\n",
    "                             .text.strip().replace('\\r', '').replace('\\n', '').split()[0])\n",
    "            except:\n",
    "                lista.append('')\n",
    "\n",
    "                #plot\n",
    "            try:\n",
    "                if plot[-7:] == '...more':\n",
    "                    lista.append(soup.find('div',id='description').contents[3].text)\n",
    "                else:\n",
    "                    lista.append(plot)\n",
    "            except:\n",
    "                lista.append('')\n",
    "\n",
    "                #number of pages\n",
    "            try:\n",
    "                lista.append(soup.find('span', itemprop='numberOfPages').text.strip().split()[0])\n",
    "            except:\n",
    "                lista.append('')\n",
    "\n",
    "                #Publishing Date\n",
    "            try:\n",
    "                a=soup.find_all('div', class_='row')[1].text\n",
    "                match_obj = re.split('Published', re.split('by', a)[0])[1]\n",
    "                lista.append(match_obj.strip())\n",
    "            except:\n",
    "                lista.append('')\n",
    "\n",
    "                #characters\n",
    "            try:\n",
    "                l1=[]\n",
    "                for d in soup.find_all('a',href=re.compile(r'/characters/*')):\n",
    "                    l1.append(d.text)\n",
    "                    s1=\",\".join(l1)\n",
    "                lista.append(s1)\n",
    "            except:\n",
    "                lista.append('')\n",
    "\n",
    "                #setting\n",
    "            try:\n",
    "                l2=[]\n",
    "                for e in soup.find_all('a',href=re.compile(r'/places/*')):\n",
    "                    l2.append(e.text)\n",
    "                    s2=\",\".join(l2)\n",
    "                lista.append(s2)\n",
    "            except:\n",
    "                lista.append('')\n",
    "\n",
    "                #URL\n",
    "            lista.append(soup.find('link')['href'].strip())\n",
    "\n",
    "            path = personalPath + str(i) + '/article_' + str(i)+ '.tsv'\n",
    "\n",
    "            with open(path, 'w', newline='',encoding=\"utf-8\") as f_output:\n",
    "                tsv_output = csv.writer(f_output, delimiter='\\t')\n",
    "                tsv_output.writerow(data)\n",
    "                tsv_output.writerow(lista)\n",
    "                f_output.close()\n",
    "        \n",
    "        else:\n",
    "            print('This book is not in english: '+ str(i))\n",
    "            \n",
    "    except:\n",
    "        print('Missing plot for book: '+ str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\giogi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateTermId(word):\n",
    "    term_id = ''\n",
    "    for c in word:\n",
    "        term_id += str(string.ascii_lowercase.index(c.lower()))\n",
    "    \n",
    "    return int(term_id)\n",
    "\n",
    "def getIds(keywords):\n",
    "    term_ids = []\n",
    "    for word in keywords:\n",
    "        try:\n",
    "            term_ids.append(voc[word])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return term_ids\n",
    "\n",
    "def getBooks(term_ids):\n",
    "    books = []\n",
    "    for ids in term_ids:\n",
    "        books.append(set(diz[ids]))\n",
    "\n",
    "    return set.intersection(*books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As suggested by the homework, we should create a file with all the following data. \n",
    "# In my opinion, better to create three files: one for the vocabulary (voc), and two for each database (diz) to be created\n",
    "# for searchEngin1 and searchEngine2\n",
    "\n",
    "books=open(\"C:/Users/giogi/Data Science/Algorithmic Methods of Data Mining/books.tsv\", 'r')    # TO BE MODIFIED\n",
    "l=books.readlines()\n",
    "i=1\n",
    "ps = PorterStemmer() \n",
    "l1=[]\n",
    "\n",
    "diz = {}\n",
    "voc = {}\n",
    "\n",
    "for line in l:\n",
    "    line1=line.split('\\t')\n",
    "    tokenizer = RegexpTokenizer(r\"[a-zA-Z]+\")\n",
    "    text_tokens = tokenizer.tokenize(line1[6])\n",
    "\n",
    "    for word in set(text_tokens):\n",
    "        if not word in stopwords.words():\n",
    "            a = ps.stem(word.lower())\n",
    "            term_id = generateTermId(a)\n",
    "            \n",
    "            if a not in voc:\n",
    "                voc[a] = term_id\n",
    "            if term_id not in diz:\n",
    "                diz[term_id] = [i]\n",
    "            else:\n",
    "                if i not in diz[term_id]:\n",
    "                    diz[term_id].append(i)\n",
    "\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchEngine1(voc, diz):\n",
    "    \n",
    "    query = input()\n",
    "    keywords = query.split()\n",
    "    \n",
    "    term_ids = getIds(keywords)\n",
    "    books = getBooks(term_ids)\n",
    "    \n",
    "    return books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love win\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1, 3, 6, 10}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searchEngine1(voc, diz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "  \n",
    "# Serializing json  \n",
    "json_object = json.dumps(diz, indent = 4) \n",
    "  \n",
    "# Writing to sample.json \n",
    "with open(\"database1.json\", \"w\") as outfile: \n",
    "    outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_object = json.dumps(voc, indent = 4) \n",
    "  \n",
    "# Writing to sample.json \n",
    "with open(\"database2.json\", \"w\") as outfile: \n",
    "    outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0            1\n",
      "1            2\n",
      "2            3\n",
      "3            4\n",
      "4            5\n",
      "         ...  \n",
      "53999    54000\n",
      "54000    54001\n",
      "54001    54002\n",
      "54002    54003\n",
      "54003    54004\n",
      "Name: 1, Length: 54004, dtype: int64 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "unknown type str32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-f71cc34d2322>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mheap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mheapq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheappush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfind_tfidf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheap\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-f71cc34d2322>\u001b[0m in \u001b[0;36mfind_tfidf\u001b[1;34m(doc_id, term_id)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfind_tfidf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mterm_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterm_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mword\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mterm_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mplot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mnum_docs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops\\common.py\u001b[0m in \u001b[0;36mnew_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops\\__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[0mrvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextract_numpy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 370\u001b[1;33m         \u001b[0mres_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomparison_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_construct_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py\u001b[0m in \u001b[0;36mcomparison_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    244\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 246\u001b[1;33m                 \u001b[0mres_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mna_arithmetic_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_cmp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mres_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py\u001b[0m in \u001b[0;36mna_arithmetic_op\u001b[1;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexpressions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_cmp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(op, a, b, use_numexpr)\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[0muse_numexpr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muse_numexpr\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0m_bool_arith_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0muse_numexpr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py\u001b[0m in \u001b[0;36m_evaluate_numexpr\u001b[1;34m(op, op_str, a, b)\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[0mb_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m         result = ne.evaluate(\n\u001b[0m\u001b[0;32m    110\u001b[0m             \u001b[1;34mf\"a_value {op_str} b_value\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m             \u001b[0mlocal_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"a_value\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0ma_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"b_value\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mb_value\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numexpr\\necompiler.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(ex, local_dict, global_dict, out, order, casting, **kwargs)\u001b[0m\n\u001b[0;32m    819\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    820\u001b[0m     \u001b[1;31m# Create a signature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 821\u001b[1;33m     signature = [(name, getType(arg)) for (name, arg) in\n\u001b[0m\u001b[0;32m    822\u001b[0m                  zip(names, arguments)]\n\u001b[0;32m    823\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numexpr\\necompiler.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    819\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    820\u001b[0m     \u001b[1;31m# Create a signature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 821\u001b[1;33m     signature = [(name, getType(arg)) for (name, arg) in\n\u001b[0m\u001b[0;32m    822\u001b[0m                  zip(names, arguments)]\n\u001b[0;32m    823\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numexpr\\necompiler.py\u001b[0m in \u001b[0;36mgetType\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m    701\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mkind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'S'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    702\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 703\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"unknown type %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    704\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: unknown type str32"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "\n",
    "\n",
    "b=pd.read_csv('index_books.tsv',sep=\"\\t\",header=None,usecols=[0,7],error_bad_lines=False)\n",
    "voc = pd.read_csv(\"vocabulary.tsv\", sep='\\t', header=None, error_bad_lines=False)\n",
    "nodigit = lambda wordslist : [word for word in wordslist if word.isalpha()]\n",
    "num_docs=len(b) \n",
    "\n",
    "def find_tfidf(doc_id,term_id):\n",
    "    print(voc[1], term_id)\n",
    "    word=voc[voc[1]==term_id][0]\n",
    "    plot=b[b[0]==doc_id][7]\n",
    "    num_docs=len(b) \n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r\"[a-zA-Z]+\") \n",
    "    text_tokens = nodigit(tokenizer.tokenize(plot))\n",
    "    tokens_without_sw = [ps.stem(word.lower()) for word in text_tokens if not word in stopwords.words()]\n",
    "    \n",
    "    tf=tokens_without_sw.count(word)/len(tokens_without_sw)\n",
    "    \n",
    "    \n",
    "    idf=math.log10(num_docs/len(diz[term_id]))\n",
    "    \n",
    "    return tf*idf\n",
    "\n",
    "\n",
    "with open('dictionary.json') as f:\n",
    "        dt = json.load(f)\n",
    "        \n",
    "d=defaultdict()   \n",
    "\n",
    "i = 0\n",
    "\n",
    "for key, values in dt.items():\n",
    "    heap=[]\n",
    "    for value in values:\n",
    "        heapq.heappush(heap, (value,find_tfidf(value,key) ))\n",
    "    \n",
    "    d[key]=heap\n",
    "\n",
    "    if i == 100:\n",
    "        break\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "# with open(\"inverted_index.json\", \"w\") as outfile: \n",
    "#     json.dump(d, outfile, indent = 4)\n",
    "    \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\giogi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv \n",
    "import re\n",
    "from langdetect import detect\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import json\n",
    "import nltk\n",
    "import re\n",
    "import heapq\n",
    "import math\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "voc = dict()\n",
    "ds = dict()\n",
    "\n",
    "with open('vocabulary.tsv') as f:\n",
    "    for col1, col2 in csv.reader(f, delimiter='\\t'):\n",
    "        voc[col1] = col2\n",
    "        \n",
    "with open('dictionary.json') as f:\n",
    "    dt = json.load(f) # dictionary\n",
    "\n",
    "with open('index_books.tsv') as f:\n",
    "    for row in csv.reader(f, delimiter='\\t'):\n",
    "        if len(row) == 13:\n",
    "            ds[row[0]] = row[7]\n",
    "\n",
    "            \n",
    "nodigit = lambda wordslist : [word for word in wordslist if word.isalpha()]\n",
    "\n",
    "result = {}\n",
    "\n",
    "c = 1\n",
    "z = 0\n",
    "\n",
    "for d in ds:\n",
    "    plot = ds[d]\n",
    "    doc_id = d\n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    tokenizer = RegexpTokenizer(r\"[a-zA-Z]+\") \n",
    "    text_tokens = nodigit(tokenizer.tokenize(plot))\n",
    "    tokens_without_sw = [ps.stem(w.lower()) for w in text_tokens if not w in stopwords.words()]\n",
    "    \n",
    "    plotLength = len(tokens_without_sw)\n",
    "    count = Counter(tokens_without_sw)\n",
    "    \n",
    "    for i in count:\n",
    "        word = i\n",
    "        freq = count[i]\n",
    "        try:\n",
    "            term_id = str(voc[word])\n",
    "            idf = math.log10( len(ds) / len( dt[term_id] ) )\n",
    "            tf = freq / plotLength\n",
    "            tfIdf = tf * idf\n",
    "            if word not in result:\n",
    "                heap = []\n",
    "                heapq.heappush(heap, (doc_id, tfIdf))\n",
    "                result[word] = heap\n",
    "            else:\n",
    "                heap = result[word]\n",
    "                heapq.heappush(heap, (doc_id, tfIdf))\n",
    "        except:\n",
    "            z += 1\n",
    "    \n",
    "    c += 1\n",
    "\n",
    "    if c == 100:\n",
    "        print(\"It's 100\")\n",
    "        \n",
    "    if c == 200:\n",
    "        print(\"It's 200\")\n",
    "        \n",
    "    if c == 300:\n",
    "        print(\"It's 300\")\n",
    "        \n",
    "    if c == 400:\n",
    "        print(\"It's 400\")\n",
    "        \n",
    "    if c == 500:\n",
    "        print(\"It's 500\")\n",
    "        \n",
    "    if c == 600:\n",
    "        print(\"It's 600\")\n",
    "        \n",
    "    if c == 700:\n",
    "        print(\"It's 700\")\n",
    "        \n",
    "    if c == 800:\n",
    "        print(\"It's 800\")\n",
    "        \n",
    "    if c == 900:\n",
    "        print(\"It's 900\")\n",
    "    \n",
    "    if c == 1000:\n",
    "        print(\"It's 1000\")\n",
    "        \n",
    "    if c == 5000:\n",
    "        print(\"It's 5000\")\n",
    "        \n",
    "    if c == 10000:\n",
    "        print(\"It's 10000\")\n",
    "        \n",
    "    if c == 15000:\n",
    "        print(\"It's 15000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
