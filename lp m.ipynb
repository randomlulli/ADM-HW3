{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\giogi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv \n",
    "import re\n",
    "from langdetect import detect\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import json\n",
    "import nltk\n",
    "import re\n",
    "import heapq\n",
    "import math\n",
    "import string\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"homework.txt\", \"w\")\n",
    "\n",
    "for i in range(1,301):\n",
    "    page = requests.get(\"https://www.goodreads.com/list/show/1.Best_Books_Ever?page=\" + str(i))\n",
    "    soup = BeautifulSoup(page.content, features=\"lxml\")\n",
    "    links = soup.find_all('a', itemprop='url', class_='bookTitle')\n",
    "    for link in links:\n",
    "        fullLink = link.get('href')\n",
    "        f.write('https://www.goodreads.com' + fullLink + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'D:/Storage file PC/Documenti/Università/Data Science/Anno 1/Semestre 1/Algorthmic Methods of Data Mining/Homeworks/HW3/st/'\n",
    "\n",
    "f = open(\"homework.txt\", \"r\")\n",
    "\n",
    "ff = f.readlines()\n",
    "\n",
    "for i in range(30000, len(ff)+1):\n",
    "    \n",
    "    folderName = \"folder-\" + str(i) + \"/\"\n",
    "    fileName = \"article_\" + str(i) + \".html\"\n",
    "    \n",
    "\n",
    "    url = ff[i-1]\n",
    "    \n",
    "    Path(path + folderName).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    page = requests.get(url)\n",
    "    code = str(page.text)\n",
    "\n",
    "    with open(path + folderName + fileName, \"w\", encoding=\"utf-8\") as z:\n",
    "        z.write(code)\n",
    "\n",
    "    z.close()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "personalPath = \"D:/Storage file PC/Documenti/Università/Data Science/Anno 1/Semestre 1/Algorthmic Methods of Data Mining/Homeworks/HW3/st/folder-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ['bookTitle', 'bookSeries', 'bookAuthors', 'ratingValue', \"ratingCount\",\\\n",
    "        \"reviewCount\", \"plot\", \"numberOfPages\", \"publishingDate\", \"characters\", \"setting\", \"url\"]\n",
    "\n",
    "personalPath = \"D:/Storage file PC/Documenti/Università/Data Science/Anno 1/Semestre 1/Algorthmic Methods of Data Mining/Homeworks/HW3/st/folder-\"\n",
    "\n",
    "for i in range(18001, 30001):\n",
    "    with open(personalPath + str(i) + \"/article_\" + str(i) + \".html\", 'rb') as html: \n",
    "        soup = BeautifulSoup(html,\"html.parser\")\n",
    "    #plot = soup.find('div',id='description').text.strip()  \n",
    "    try:\n",
    "        plot = soup.find('div',id='description').text.strip()\n",
    "        if detect(plot)=='en':\n",
    "            lista=[]\n",
    "\n",
    "            #title\n",
    "            try:\n",
    "                lista.append(soup.find('h1').text.strip())\n",
    "            except:\n",
    "                lista.append('')\n",
    "\n",
    "                #bookseries\n",
    "            try:\n",
    "                lista.append(soup.find('h2',id='bookSeries').text.strip())\n",
    "            except:\n",
    "                lista.append('')\n",
    "\n",
    "                #author name\n",
    "            try:\n",
    "                lista.append(soup.find('a',class_='authorName').text.strip())\n",
    "            except:\n",
    "                lista.append('')\n",
    "\n",
    "                #rating value\n",
    "            try:\n",
    "                lista.append(soup.find('span', itemprop='ratingValue').text.strip())\n",
    "            except:\n",
    "                lista.append('')\n",
    "\n",
    "                #ratingCount\n",
    "            try:\n",
    "                lista.append(soup.find_all('a',class_='gr-hyperlink',href='#other_reviews')[0]\\\n",
    "                             .text.strip().replace('\\r', '').replace('\\n', '').split()[0])\n",
    "            except:\n",
    "                lista.append('')\n",
    "\n",
    "                #reviewCount\n",
    "            try:\n",
    "                lista.append(soup.find_all('a',class_='gr-hyperlink',href='#other_reviews')[1]\\\n",
    "                             .text.strip().replace('\\r', '').replace('\\n', '').split()[0])\n",
    "            except:\n",
    "                lista.append('')\n",
    "\n",
    "                #plot\n",
    "            try:\n",
    "                if plot[-7:] == '...more':\n",
    "                    lista.append(soup.find('div',id='description').contents[3].text)\n",
    "                else:\n",
    "                    lista.append(plot)\n",
    "            except:\n",
    "                lista.append('')\n",
    "\n",
    "                #number of pages\n",
    "            try:\n",
    "                lista.append(soup.find('span', itemprop='numberOfPages').text.strip().split()[0])\n",
    "            except:\n",
    "                lista.append('')\n",
    "\n",
    "                #Publishing Date\n",
    "            try:\n",
    "                a=soup.find_all('div', class_='row')[1].text\n",
    "                match_obj = re.split('Published', re.split('by', a)[0])[1]\n",
    "                lista.append(match_obj.strip())\n",
    "            except:\n",
    "                lista.append('')\n",
    "\n",
    "                #characters\n",
    "            try:\n",
    "                l1=[]\n",
    "                for d in soup.find_all('a',href=re.compile(r'/characters/*')):\n",
    "                    l1.append(d.text)\n",
    "                    s1=\",\".join(l1)\n",
    "                lista.append(s1)\n",
    "            except:\n",
    "                lista.append('')\n",
    "\n",
    "                #setting\n",
    "            try:\n",
    "                l2=[]\n",
    "                for e in soup.find_all('a',href=re.compile(r'/places/*')):\n",
    "                    l2.append(e.text)\n",
    "                    s2=\",\".join(l2)\n",
    "                lista.append(s2)\n",
    "            except:\n",
    "                lista.append('')\n",
    "\n",
    "                #URL\n",
    "            lista.append(soup.find('link')['href'].strip())\n",
    "\n",
    "            path = personalPath + str(i) + '/article_' + str(i)+ '.tsv'\n",
    "\n",
    "            with open(path, 'w', newline='',encoding=\"utf-8\") as f_output:\n",
    "                tsv_output = csv.writer(f_output, delimiter='\\t')\n",
    "                tsv_output.writerow(data)\n",
    "                tsv_output.writerow(lista)\n",
    "                f_output.close()\n",
    "        \n",
    "        else:\n",
    "            print('This book is not in english: '+ str(i))\n",
    "            \n",
    "    except:\n",
    "        print('Missing plot for book: '+ str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "index = 1\n",
    "\n",
    "new_file = open('index_books.tsv', 'w')\n",
    "\n",
    "while i <= 30000:\n",
    "    try:\n",
    "        art_f = open(personalPath + str(i) + \"/article_\" + str(i) + \".tsv\", 'r')\n",
    "        art = art_f.readlines()[1]\n",
    "        new_file.write(str(index) + \"\\t\" + art)\n",
    "        art_f.close()\n",
    "        i += 1\n",
    "        index += 1\n",
    "    except:\n",
    "        i += 1\n",
    "\n",
    "new_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodigit = lambda wordslist : [word for word in wordslist if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "f = open(\"books.tsv\", 'r')\n",
    "books = f.readlines()\n",
    "\n",
    "new_file = open('vocabolary.tsv', 'w')\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "term_id = 1\n",
    "document_id = 1\n",
    "\n",
    "vocabolary = dict()\n",
    "diz = defaultdict(set)\n",
    "\n",
    "for book in books:\n",
    "    tokenizer = RegexpTokenizer(r\"[a-zA-Z]+\") \n",
    "    text_tokens = nodigit(tokenizer.tokenize(book.split('\\t')[6]))\n",
    "    tokens_without_sw = {word for word in text_tokens if not word in stopwords.words()}\n",
    "    for word in tokens_without_sw:\n",
    "        w = ps.stem(word.lower())\n",
    "        if w not in vocabolary:\n",
    "            vocabolary[w] = term_id\n",
    "            diz[term_id].add(document_id)\n",
    "            new_file.write(w + \"\\t\" + str(term_id) + '\\n')\n",
    "            term_id += 1\n",
    "        else:\n",
    "            diz[vocabolary[w]].add(document_id)\n",
    "    print('Finished document ' + str(document_id))\n",
    "    document_id += 1\n",
    "    \n",
    "new_file.close()\n",
    "f.close()\n",
    "\n",
    "\n",
    "with open(\"dictionary.json\", \"w\") as outfile: \n",
    "    json.dump(dict(zip(diz.keys(), map(list, diz.values()))), outfile, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_csv('index_books.tsv', header=None, sep='\\t', usecols=[0,1,7,12])\n",
    "\n",
    "ds.rename(columns={0:'index', 1:'bookTitle', 7:'plot', 12:'url'}, inplace=True)\n",
    "\n",
    "voc = dict()\n",
    "with open('vocabolary.tsv') as f:\n",
    "    for col1, col2 in csv.reader(f, delimiter='\\t'):\n",
    "        voc[col1] = col2\n",
    "        \n",
    "with open('dictionary.json') as f:\n",
    "    dt = json.load(f) # dictionary\n",
    "\n",
    "\n",
    "def query(q):\n",
    "\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    q = q.strip().split() # input from user\n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    q = [ps.stem(w).lower() for w in q]\n",
    "\n",
    "    # elaborate query\n",
    "    \n",
    "    # take term_id(s)\n",
    "    term = list()\n",
    "    for w in q:\n",
    "        try:\n",
    "            term.append(voc[w])\n",
    "        except:\n",
    "            pass\n",
    "    # matching documents\n",
    "    if len(term):\n",
    "        doc = set(dt[term[0]])\n",
    "        for i in range(1, len(term)):\n",
    "            doc = doc.intersection(dt[term[i]])\n",
    "        # take row from books\n",
    "        return ds[ds['index'].isin(list(doc))][['index', 'bookTitle', 'plot', 'url']].head()\n",
    "    else:\n",
    "        return \"There aren't documents for each word of this query\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = dict()\n",
    "with open('vocabolary.tsv') as f:\n",
    "    for col1, col2 in csv.reader(f, delimiter='\\t'):\n",
    "        voc[col1] = col2\n",
    "        \n",
    "with open('dictionary.json') as f:\n",
    "    dt = json.load(f) # dictionary\n",
    "    \n",
    "ds = dict()\n",
    "with open('index_books.tsv') as f:\n",
    "    for row in csv.reader(f, delimiter='\\t'):\n",
    "        if len(row) == 13:\n",
    "            ds[row[0]] = row[7]\n",
    "\n",
    "\n",
    "result = defaultdict(list)\n",
    "term_idf = defaultdict(float)\n",
    "\n",
    "for doc_id in ds:\n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    tokenizer = RegexpTokenizer(r\"[a-zA-Z]+\") \n",
    "    text_tokens = nodigit(tokenizer.tokenize(ds[doc_id]))\n",
    "    tokens_without_sw = [ps.stem(w.lower()) for w in text_tokens if not w in stopwords.words()]\n",
    "    \n",
    "    plotLength = len(tokens_without_sw)\n",
    "    count = Counter(tokens_without_sw)\n",
    "    \n",
    "    for word in count:\n",
    "        freq = count[word]\n",
    "        try:\n",
    "            term_id = str(voc[word])\n",
    "            idf = 1.0 + math.log( float(len(ds)) / len( dt[term_id] ) )\n",
    "            tf = freq / plotLength\n",
    "            tfIdf = tf * idf\n",
    "\n",
    "            \n",
    "            heapq.heappush(result[term_id], (tfIdf, doc_id))\n",
    "            term_idf[term_id] = idf\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "inv_ind = defaultdict(list)\n",
    "for term, tup_list in result.items():\n",
    "    for tup in tup_list:\n",
    "        inv_ind[term].append( (int(tup[1]), tup[0]) )\n",
    "\n",
    "\n",
    "with open(\"inverted_index.json\", \"w\") as outfile: \n",
    "    json.dump(result, outfile, indent = 4)\n",
    "\n",
    "with open(\"term_idf.json\", \"w\") as outfile: \n",
    "    json.dump(term_idf, outfile, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('term_idf.json') as f:\n",
    "    term_idf = json.load(f)\n",
    "\n",
    "with open('inverted_index2.json') as f:\n",
    "    inverted = json.load(f)\n",
    "    \n",
    "inv_ind = defaultdict(dict)\n",
    "for term in inverted:\n",
    "    for t in inverted[term]:\n",
    "        inv_ind[term][t[0]] = t[1]\n",
    "\n",
    "dot = lambda x, y : sum(xi*yi for xi, yi in zip(x, y))\n",
    "square = lambda x : [v**2 for v in x]\n",
    "det = lambda x : math.sqrt(sum(square(x)))\n",
    "    \n",
    "def similarity(q):\n",
    "    ps = PorterStemmer()\n",
    "    # execute query\n",
    "    err = \"There aren't documents for each word of this query\"\n",
    "    q_result = query(q)\n",
    "    if not isinstance(q_result, str):\n",
    "        q = q.strip().split() # input from user\n",
    "        q = [ps.stem(w).lower() for w in q]\n",
    "        # create a list of ifidf of terms\n",
    "        term_ifidf = list()\n",
    "        tf = 1/len(q)\n",
    "        for w in q:\n",
    "            term_ifidf += [term_idf[voc[w]]*tf]\n",
    "        # create a list of ifidf of document\n",
    "        doc_ifidf = defaultdict(list)\n",
    "        for d_id in q_result['index']:\n",
    "            for w in q:\n",
    "                doc_ifidf[d_id].append(inv_ind[voc[w]][d_id])\n",
    "        #compare value and calculate similarity\n",
    "        cos_sim = list()\n",
    "        det_q = det(term_ifidf)\n",
    "        for doc in q_result['index']:\n",
    "            prod = dot(doc_ifidf[doc], term_ifidf)\n",
    "            det_doc = det(doc_ifidf[doc])\n",
    "            cos_sim += [(prod / (det_q * det_doc))]\n",
    "        q_result['similarity'] = cos_sim\n",
    "        return q_result.sort_values(by=['similarity', 'index'], ascending=False)[['index','bookTitle', 'plot', 'url', 'similarity']].head()\n",
    "    else:\n",
    "        return err\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normDate(x):\n",
    "    r = list()\n",
    "    for e in x.fillna(''):\n",
    "        v = e.replace('th', ' ').replace('nd', ' ').replace('st', ' ').replace('rd', ' ')\\\n",
    "                       .translate(str.maketrans('', '', string.punctuation)).split()\n",
    "        if len(v) > 3 or len(v) == 0:\n",
    "            r.append(None)\n",
    "        else:\n",
    "            r.append(v)\n",
    "    return r\n",
    "\n",
    "normString = lambda x : [i.translate(str.maketrans('', '', string.punctuation)).lower().split() if len(i) > 0 else None for i in x.fillna('') ]\n",
    "normFloat = lambda x : [float(str(i).replace(',','')) if i == i else None for i in x]\n",
    "normInt = lambda x : [int(i.replace(',','')) if i == i else None for i in x]\n",
    "normPages = lambda x : [int(i.replace(',','')) if i == i and i.isnumeric() else None for i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 952: expected 13 fields, saw 15\\nSkipping line 1018: expected 13 fields, saw 15\\nSkipping line 1070: expected 13 fields, saw 15\\nSkipping line 1090: expected 13 fields, saw 15\\nSkipping line 1144: expected 13 fields, saw 15\\nSkipping line 1229: expected 13 fields, saw 15\\nSkipping line 1337: expected 13 fields, saw 15\\nSkipping line 1518: expected 13 fields, saw 15\\nSkipping line 1837: expected 13 fields, saw 15\\nSkipping line 1955: expected 13 fields, saw 15\\nSkipping line 2075: expected 13 fields, saw 15\\nSkipping line 2125: expected 13 fields, saw 15\\nSkipping line 2136: expected 13 fields, saw 15\\nSkipping line 2550: expected 13 fields, saw 15\\nSkipping line 2588: expected 13 fields, saw 15\\nSkipping line 2630: expected 13 fields, saw 15\\nSkipping line 2638: expected 13 fields, saw 15\\nSkipping line 2716: expected 13 fields, saw 15\\nSkipping line 2735: expected 13 fields, saw 15\\nSkipping line 2851: expected 13 fields, saw 15\\nSkipping line 2911: expected 13 fields, saw 15\\nSkipping line 2927: expected 13 fields, saw 15\\nSkipping line 3186: expected 13 fields, saw 15\\nSkipping line 3390: expected 13 fields, saw 15\\nSkipping line 3529: expected 13 fields, saw 15\\nSkipping line 3553: expected 13 fields, saw 15\\nSkipping line 3578: expected 13 fields, saw 15\\nSkipping line 4013: expected 13 fields, saw 15\\nSkipping line 4171: expected 13 fields, saw 15\\nSkipping line 4462: expected 13 fields, saw 15\\nSkipping line 4646: expected 13 fields, saw 15\\nSkipping line 4831: expected 13 fields, saw 15\\nSkipping line 4887: expected 13 fields, saw 15\\nSkipping line 5085: expected 13 fields, saw 15\\nSkipping line 5424: expected 13 fields, saw 15\\nSkipping line 5539: expected 13 fields, saw 15\\nSkipping line 5686: expected 13 fields, saw 15\\nSkipping line 5713: expected 13 fields, saw 15\\nSkipping line 5810: expected 13 fields, saw 15\\nSkipping line 5960: expected 13 fields, saw 15\\nSkipping line 5976: expected 13 fields, saw 15\\nSkipping line 6011: expected 13 fields, saw 15\\nSkipping line 6050: expected 13 fields, saw 15\\nSkipping line 6318: expected 13 fields, saw 15\\nSkipping line 6368: expected 13 fields, saw 15\\nSkipping line 6466: expected 13 fields, saw 15\\nSkipping line 6479: expected 13 fields, saw 15\\nSkipping line 6599: expected 13 fields, saw 15\\nSkipping line 6671: expected 13 fields, saw 15\\nSkipping line 6792: expected 13 fields, saw 15\\nSkipping line 7021: expected 13 fields, saw 15\\nSkipping line 7189: expected 13 fields, saw 15\\nSkipping line 7282: expected 13 fields, saw 15\\nSkipping line 7372: expected 13 fields, saw 15\\nSkipping line 7549: expected 13 fields, saw 18\\nSkipping line 7568: expected 13 fields, saw 15\\nSkipping line 7578: expected 13 fields, saw 15\\nSkipping line 7754: expected 13 fields, saw 15\\nSkipping line 7891: expected 13 fields, saw 15\\nSkipping line 7901: expected 13 fields, saw 19\\nSkipping line 8101: expected 13 fields, saw 15\\nSkipping line 8133: expected 13 fields, saw 15\\nSkipping line 8324: expected 13 fields, saw 15\\nSkipping line 8601: expected 13 fields, saw 15\\nSkipping line 8613: expected 13 fields, saw 15\\nSkipping line 8626: expected 13 fields, saw 15\\nSkipping line 8631: expected 13 fields, saw 15\\nSkipping line 8674: expected 13 fields, saw 15\\nSkipping line 8810: expected 13 fields, saw 15\\nSkipping line 8894: expected 13 fields, saw 15\\nSkipping line 8910: expected 13 fields, saw 15\\nSkipping line 8989: expected 13 fields, saw 15\\nSkipping line 8996: expected 13 fields, saw 15\\nSkipping line 9218: expected 13 fields, saw 17\\nSkipping line 9382: expected 13 fields, saw 15\\nSkipping line 9510: expected 13 fields, saw 15\\nSkipping line 9771: expected 13 fields, saw 15\\nSkipping line 9959: expected 13 fields, saw 15\\nSkipping line 10025: expected 13 fields, saw 15\\nSkipping line 10089: expected 13 fields, saw 15\\nSkipping line 10121: expected 13 fields, saw 15\\nSkipping line 10220: expected 13 fields, saw 15\\nSkipping line 10248: expected 13 fields, saw 15\\nSkipping line 10336: expected 13 fields, saw 15\\nSkipping line 10367: expected 13 fields, saw 15\\nSkipping line 10389: expected 13 fields, saw 15\\nSkipping line 10489: expected 13 fields, saw 15\\nSkipping line 10659: expected 13 fields, saw 15\\nSkipping line 10866: expected 13 fields, saw 15\\nSkipping line 10968: expected 13 fields, saw 15\\nSkipping line 11038: expected 13 fields, saw 15\\nSkipping line 11042: expected 13 fields, saw 15\\nSkipping line 11093: expected 13 fields, saw 15\\nSkipping line 11254: expected 13 fields, saw 15\\nSkipping line 11301: expected 13 fields, saw 15\\nSkipping line 11306: expected 13 fields, saw 15\\nSkipping line 11348: expected 13 fields, saw 15\\nSkipping line 12002: expected 13 fields, saw 15\\nSkipping line 12383: expected 13 fields, saw 15\\nSkipping line 12478: expected 13 fields, saw 15\\nSkipping line 12549: expected 13 fields, saw 15\\nSkipping line 12628: expected 13 fields, saw 15\\nSkipping line 12668: expected 13 fields, saw 15\\nSkipping line 12836: expected 13 fields, saw 15\\nSkipping line 12848: expected 13 fields, saw 15\\nSkipping line 12992: expected 13 fields, saw 15\\nSkipping line 13109: expected 13 fields, saw 15\\nSkipping line 13148: expected 13 fields, saw 15\\nSkipping line 13187: expected 13 fields, saw 15\\nSkipping line 13321: expected 13 fields, saw 15\\nSkipping line 13589: expected 13 fields, saw 15\\nSkipping line 13606: expected 13 fields, saw 15\\nSkipping line 13626: expected 13 fields, saw 15\\nSkipping line 13872: expected 13 fields, saw 15\\nSkipping line 13953: expected 13 fields, saw 15\\nSkipping line 14026: expected 13 fields, saw 15\\nSkipping line 14052: expected 13 fields, saw 15\\nSkipping line 14182: expected 13 fields, saw 15\\nSkipping line 14212: expected 13 fields, saw 15\\nSkipping line 14222: expected 13 fields, saw 15\\nSkipping line 14288: expected 13 fields, saw 15\\nSkipping line 14394: expected 13 fields, saw 15\\nSkipping line 14423: expected 13 fields, saw 36\\nSkipping line 14467: expected 13 fields, saw 15\\nSkipping line 14585: expected 13 fields, saw 15\\nSkipping line 14608: expected 13 fields, saw 15\\nSkipping line 14677: expected 13 fields, saw 15\\nSkipping line 14683: expected 13 fields, saw 15\\nSkipping line 14698: expected 13 fields, saw 15\\nSkipping line 14700: expected 13 fields, saw 15\\nSkipping line 14741: expected 13 fields, saw 15\\nSkipping line 14756: expected 13 fields, saw 15\\nSkipping line 14760: expected 13 fields, saw 15\\nSkipping line 14795: expected 13 fields, saw 15\\nSkipping line 14819: expected 13 fields, saw 15\\nSkipping line 14903: expected 13 fields, saw 15\\nSkipping line 14908: expected 13 fields, saw 15\\nSkipping line 14972: expected 13 fields, saw 15\\nSkipping line 15057: expected 13 fields, saw 15\\nSkipping line 15099: expected 13 fields, saw 15\\nSkipping line 15157: expected 13 fields, saw 15\\nSkipping line 15160: expected 13 fields, saw 15\\nSkipping line 15491: expected 13 fields, saw 15\\nSkipping line 15588: expected 13 fields, saw 15\\nSkipping line 15606: expected 13 fields, saw 15\\nSkipping line 15629: expected 13 fields, saw 15\\nSkipping line 15737: expected 13 fields, saw 15\\nSkipping line 15903: expected 13 fields, saw 15\\nSkipping line 15947: expected 13 fields, saw 15\\nSkipping line 15981: expected 13 fields, saw 15\\nSkipping line 16034: expected 13 fields, saw 15\\nSkipping line 16105: expected 13 fields, saw 15\\nSkipping line 16153: expected 13 fields, saw 15\\nSkipping line 16385: expected 13 fields, saw 15\\nSkipping line 16409: expected 13 fields, saw 15\\nSkipping line 16439: expected 13 fields, saw 15\\nSkipping line 16552: expected 13 fields, saw 15\\nSkipping line 16571: expected 13 fields, saw 15\\nSkipping line 16617: expected 13 fields, saw 15\\nSkipping line 16685: expected 13 fields, saw 15\\nSkipping line 16782: expected 13 fields, saw 15\\nSkipping line 16785: expected 13 fields, saw 15\\nSkipping line 16815: expected 13 fields, saw 15\\nSkipping line 16851: expected 13 fields, saw 15\\nSkipping line 16870: expected 13 fields, saw 15\\nSkipping line 17028: expected 13 fields, saw 15\\nSkipping line 17065: expected 13 fields, saw 15\\nSkipping line 17352: expected 13 fields, saw 15\\nSkipping line 17398: expected 13 fields, saw 18\\nSkipping line 17429: expected 13 fields, saw 15\\nSkipping line 17562: expected 13 fields, saw 15\\nSkipping line 17609: expected 13 fields, saw 15\\nSkipping line 17634: expected 13 fields, saw 15\\nSkipping line 17652: expected 13 fields, saw 15\\nSkipping line 17670: expected 13 fields, saw 15\\nSkipping line 17693: expected 13 fields, saw 15\\nSkipping line 17724: expected 13 fields, saw 15\\nSkipping line 17817: expected 13 fields, saw 15\\nSkipping line 17843: expected 13 fields, saw 15\\nSkipping line 17906: expected 13 fields, saw 15\\nSkipping line 17947: expected 13 fields, saw 15\\nSkipping line 17985: expected 13 fields, saw 15\\nSkipping line 18027: expected 13 fields, saw 15\\nSkipping line 18179: expected 13 fields, saw 15\\nSkipping line 18364: expected 13 fields, saw 15\\nSkipping line 18781: expected 13 fields, saw 15\\nSkipping line 18801: expected 13 fields, saw 15\\nSkipping line 18947: expected 13 fields, saw 15\\nSkipping line 19028: expected 13 fields, saw 15\\nSkipping line 19240: expected 13 fields, saw 15\\nSkipping line 19457: expected 13 fields, saw 15\\nSkipping line 19579: expected 13 fields, saw 15\\nSkipping line 19774: expected 13 fields, saw 15\\nSkipping line 20088: expected 13 fields, saw 15\\nSkipping line 20148: expected 13 fields, saw 15\\nSkipping line 20355: expected 13 fields, saw 15\\nSkipping line 20457: expected 13 fields, saw 15\\nSkipping line 20665: expected 13 fields, saw 19\\nSkipping line 20917: expected 13 fields, saw 15\\nSkipping line 20928: expected 13 fields, saw 15\\n'\n"
     ]
    }
   ],
   "source": [
    "ds = pd.read_csv('index_books.tsv', header=None, sep='\\t', error_bad_lines = False)\n",
    "\n",
    "\n",
    "ds.rename(columns={0:'index', 1:'bookTitle', 2:'bookSeries', 3:'bookAuthors', 4:'ratingValue', 5:'ratingCount', 6:'reviewCount', 7:'plot', 8:'numberOfPages', 9:'publishingDate', 10:'characters', 11:'setting', 12:'url'}, inplace=True)\n",
    "\n",
    "normstring = lambda x : [i.translate(str.maketrans('', '', string.punctuation)).split() for i in x.fillna('') if len(i)>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dopo di questa c'è una version alternativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normDate(x):\n",
    "    r = list()\n",
    "    for e in x.fillna(''):\n",
    "        v = e.replace('th', ' ').replace('nd', ' ').replace('st', ' ').replace('rd', ' ')\\\n",
    "                       .translate(str.maketrans('', '', string.punctuation)).split()\n",
    "        if len(v) <= 3 or len(v) != 0:\n",
    "            r.append(v)\n",
    "    return r\n",
    "\n",
    "\n",
    "def truncate(n):\n",
    "    n = str(n).replace('',' ').split()\n",
    "    n.reverse()\n",
    "    for i in range(1, len(n)):\n",
    "        v = int(n[i])\n",
    "        if int(n[i-1]) >= 5:\n",
    "            n[i] = str(v+1)\n",
    "    n.reverse()\n",
    "    return int(n[0] + '0'*(len(n)-1))\n",
    "\n",
    "normString = lambda x : [i.translate(str.maketrans('', '', string.punctuation)).lower().split() if len(i) > 0 else None for i in x.fillna('')  ]\n",
    "normFloat = lambda x : [round(float(str(i).replace(',',''))) if i == i else 0 for i in x]\n",
    "normInt = lambda x : [int(truncate(i.replace(',',''))) if i == i else 0 for i in x]\n",
    "normPages = lambda x : [int(truncate(i.replace(',',''))) if i == i and i.isnumeric() else 0 for i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ds = pd.DataFrame(ds['index'])\n",
    "n_ds['bookTitle'] = normString(ds['bookTitle'])\n",
    "n_ds['bookSeries'] = normString(ds['bookSeries'])\n",
    "n_ds['bookAuthors'] = normString(ds['bookAuthors'])\n",
    "n_ds['ratingValue'] = normFloat(ds['ratingValue'])\n",
    "n_ds['ratingCount'] = normInt(ds['ratingCount'])\n",
    "n_ds['reviewCount'] = normInt(ds['reviewCount'])\n",
    "n_ds['plot'] = ds['plot']\n",
    "n_ds['numberOfPages'] = normPages(ds['numberOfPages'])\n",
    "n_ds['publishingDate'] = normDate(ds['publishingDate'])\n",
    "n_ds['characters'] = normString(ds['characters'])\n",
    "n_ds['setting'] = normString(ds['setting'])\n",
    "n_ds['url'] = ds['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(q):\n",
    "    # execute query\n",
    "    err = \"There aren't documents for each word of this query\"\n",
    "    qs = re.sub('\\d', '', q.translate(str.maketrans('', '', string.punctuation)).lower())\n",
    "    q_result = similarity(qs)\n",
    "    if not isinstance(q_result, str):\n",
    "        q = q.strip().split()\n",
    "        q = [w.lower() for w in q]\n",
    "        # power up of the score\n",
    "        doc_score = []\n",
    "        for doc_id in q_result['index']:\n",
    "            score = q_result[q_result['index'] == doc_id]['similarity'].to_list()[0]\n",
    "            # calculate score\n",
    "            for w in q:\n",
    "                t = n_ds[n_ds['index']==doc_id]['bookTitle'].to_list()[0]\n",
    "                s = n_ds[n_ds['index']==doc_id]['bookSeries'].to_list()[0]\n",
    "                a = n_ds[n_ds['index']==doc_id]['bookAuthors'].to_list()[0]\n",
    "                c = n_ds[n_ds['index']==doc_id]['characters'].to_list()[0]\n",
    "                st = n_ds[n_ds['index']==doc_id]['setting'].to_list()[0]\n",
    "                d = n_ds[n_ds['index']==doc_id]['publishingDate'].to_list()[0]\n",
    "                if t != None and w in t:\n",
    "                    score += (1/len(t))*2\n",
    "                if s != None and w in s:\n",
    "                    score += (1/len(s))*1.5\n",
    "                if a != None and w in a:\n",
    "                    score += (1/len(a))*2\n",
    "                if c != None and w in c:\n",
    "                    score += (1/len(c))*1.5\n",
    "                if st != None and w in st:\n",
    "                    score += (1/len(st))\n",
    "                if d != None and w in d:\n",
    "                    score += (1/len(d))\n",
    "                if w.isnumeric():\n",
    "                    if truncate(w) == n_ds[n_ds['index']==doc_id]['ratingCount'].to_list()[0]:\n",
    "                        score += 0.5\n",
    "                    if truncate(w) == n_ds[n_ds['index']==doc_id]['numberOfPages'].to_list()[0]:\n",
    "                        score += 0.5\n",
    "                    if truncate(w) == n_ds[n_ds['index']==doc_id]['reviewCount'].to_list()[0]:\n",
    "                        score += 0.5\n",
    "                    if round(float(w)) == n_ds[n_ds['index']==doc_id]['ratingValue'].to_list()[0]:\n",
    "                        score += 0.5\n",
    "            heapq.heappush(doc_score, (score, doc_id))\n",
    "        order_doc_id = [i[1] for i in doc_score]\n",
    "        order_score = [i[0] for i in doc_score]\n",
    "        r = pd.DataFrame(q_result[q_result['index']==order_doc_id[0]][['index', 'bookTitle', 'plot', 'url']])\n",
    "        for d_id in range(1, len(order_doc_id)):\n",
    "            r = r.append(q_result[q_result['index']==order_doc_id[d_id]][['index', 'bookTitle', 'plot', 'url']])\n",
    "        r['score'] = order_score\n",
    "        return r.sort_values(by=['score', 'index'], ascending=False)[['index', 'bookTitle', 'plot', 'url', 'score']].head()\n",
    "    else:\n",
    "        return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'query' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-1225a25e1029>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'winter love games 4.5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-38a2358c849f>\u001b[0m in \u001b[0;36msearch\u001b[1;34m(q)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"There aren't documents for each word of this query\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mqs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\d'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaketrans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mq_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-1b3be3a2c1da>\u001b[0m in \u001b[0;36msimilarity\u001b[1;34m(q)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m# execute query\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"There aren't documents for each word of this query\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mq_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# input from user\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'query' is not defined"
     ]
    }
   ],
   "source": [
    "search('winter love games 4.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
