{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Luca\n",
      "[nltk_data]     Palluzzi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv \n",
    "import re\n",
    "from langdetect import detect\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import json\n",
    "import nltk\n",
    "import re\n",
    "import heapq\n",
    "import math\n",
    "import string\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"homework.txt\", \"w\")\n",
    "\n",
    "for i in range(1,301):\n",
    "    page = requests.get(\"https://www.goodreads.com/list/show/1.Best_Books_Ever?page=\" + str(i))\n",
    "    soup = BeautifulSoup(page.content, features=\"lxml\")\n",
    "    links = soup.find_all('a', itemprop='url', class_='bookTitle')\n",
    "    for link in links:\n",
    "        fullLink = link.get('href')\n",
    "        f.write('https://www.goodreads.com' + fullLink + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'D:/Storage file PC/Documenti/Università/Data Science/Anno 1/Semestre 1/Algorthmic Methods of Data Mining/Homeworks/HW3/st/'\n",
    "\n",
    "f = open(\"homework.txt\", \"r\")\n",
    "\n",
    "ff = f.readlines()\n",
    "\n",
    "for i in range(30000, len(ff)+1):\n",
    "    \n",
    "    folderName = \"folder-\" + str(i) + \"/\"\n",
    "    fileName = \"article_\" + str(i) + \".html\"\n",
    "    \n",
    "\n",
    "    url = ff[i-1]\n",
    "    \n",
    "    Path(path + folderName).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    page = requests.get(url)\n",
    "    code = str(page.text)\n",
    "\n",
    "    with open(path + folderName + fileName, \"w\", encoding=\"utf-8\") as z:\n",
    "        z.write(code)\n",
    "\n",
    "    z.close()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "personalPath = \"D:/Storage file PC/Documenti/Università/Data Science/Anno 1/Semestre 1/Algorthmic Methods of Data Mining/Homeworks/HW3/st/folder-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ['bookTitle', 'bookSeries', 'bookAuthors', 'ratingValue', \"ratingCount\",\\\n",
    "        \"reviewCount\", \"plot\", \"numberOfPages\", \"publishingDate\", \"characters\", \"setting\", \"url\"]\n",
    "\n",
    "personalPath = \"D:/Storage file PC/Documenti/Università/Data Science/Anno 1/Semestre 1/Algorthmic Methods of Data Mining/Homeworks/HW3/st/folder-\"\n",
    "\n",
    "for i in range(18001, 30001):\n",
    "    with open(personalPath + str(i) + \"/article_\" + str(i) + \".html\", 'rb') as html: \n",
    "        soup = BeautifulSoup(html,\"html.parser\")\n",
    "    #plot = soup.find('div',id='description').text.strip()  \n",
    "    try:\n",
    "        plot = soup.find('div',id='description').text.strip()\n",
    "        if detect(plot)=='en':\n",
    "            lista=[]\n",
    "\n",
    "            #title\n",
    "            try:\n",
    "                lista.append(soup.find('h1').text.strip())\n",
    "            except:\n",
    "                lista.append('')\n",
    "\n",
    "                #bookseries\n",
    "            try:\n",
    "                lista.append(soup.find('h2',id='bookSeries').text.strip())\n",
    "            except:\n",
    "                lista.append('')\n",
    "\n",
    "                #author name\n",
    "            try:\n",
    "                lista.append(soup.find('a',class_='authorName').text.strip())\n",
    "            except:\n",
    "                lista.append('')\n",
    "\n",
    "                #rating value\n",
    "            try:\n",
    "                lista.append(soup.find('span', itemprop='ratingValue').text.strip())\n",
    "            except:\n",
    "                lista.append('')\n",
    "\n",
    "                #ratingCount\n",
    "            try:\n",
    "                lista.append(soup.find_all('a',class_='gr-hyperlink',href='#other_reviews')[0]\\\n",
    "                             .text.strip().replace('\\r', '').replace('\\n', '').split()[0])\n",
    "            except:\n",
    "                lista.append('')\n",
    "\n",
    "                #reviewCount\n",
    "            try:\n",
    "                lista.append(soup.find_all('a',class_='gr-hyperlink',href='#other_reviews')[1]\\\n",
    "                             .text.strip().replace('\\r', '').replace('\\n', '').split()[0])\n",
    "            except:\n",
    "                lista.append('')\n",
    "\n",
    "                #plot\n",
    "            try:\n",
    "                if plot[-7:] == '...more':\n",
    "                    lista.append(soup.find('div',id='description').contents[3].text)\n",
    "                else:\n",
    "                    lista.append(plot)\n",
    "            except:\n",
    "                lista.append('')\n",
    "\n",
    "                #number of pages\n",
    "            try:\n",
    "                lista.append(soup.find('span', itemprop='numberOfPages').text.strip().split()[0])\n",
    "            except:\n",
    "                lista.append('')\n",
    "\n",
    "                #Publishing Date\n",
    "            try:\n",
    "                a=soup.find_all('div', class_='row')[1].text\n",
    "                match_obj = re.split('Published', re.split('by', a)[0])[1]\n",
    "                lista.append(match_obj.strip())\n",
    "            except:\n",
    "                lista.append('')\n",
    "\n",
    "                #characters\n",
    "            try:\n",
    "                l1=[]\n",
    "                for d in soup.find_all('a',href=re.compile(r'/characters/*')):\n",
    "                    l1.append(d.text)\n",
    "                    s1=\",\".join(l1)\n",
    "                lista.append(s1)\n",
    "            except:\n",
    "                lista.append('')\n",
    "\n",
    "                #setting\n",
    "            try:\n",
    "                l2=[]\n",
    "                for e in soup.find_all('a',href=re.compile(r'/places/*')):\n",
    "                    l2.append(e.text)\n",
    "                    s2=\",\".join(l2)\n",
    "                lista.append(s2)\n",
    "            except:\n",
    "                lista.append('')\n",
    "\n",
    "                #URL\n",
    "            lista.append(soup.find('link')['href'].strip())\n",
    "\n",
    "            path = personalPath + str(i) + '/article_' + str(i)+ '.tsv'\n",
    "\n",
    "            with open(path, 'w', newline='',encoding=\"utf-8\") as f_output:\n",
    "                tsv_output = csv.writer(f_output, delimiter='\\t')\n",
    "                tsv_output.writerow(data)\n",
    "                tsv_output.writerow(lista)\n",
    "                f_output.close()\n",
    "        \n",
    "        else:\n",
    "            print('This book is not in english: '+ str(i))\n",
    "            \n",
    "    except:\n",
    "        print('Missing plot for book: '+ str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "index = 1\n",
    "\n",
    "new_file = open('index_books.tsv', 'w')\n",
    "\n",
    "while i <= 30000:\n",
    "    try:\n",
    "        art_f = open(personalPath + str(i) + \"/article_\" + str(i) + \".tsv\", 'r')\n",
    "        art = art_f.readlines()[1]\n",
    "        new_file.write(str(index) + \"\\t\" + art)\n",
    "        art_f.close()\n",
    "        i += 1\n",
    "        index += 1\n",
    "    except:\n",
    "        i += 1\n",
    "\n",
    "new_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodigit = lambda wordslist : [word for word in wordslist if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "f = open(\"books.tsv\", 'r')\n",
    "books = f.readlines()\n",
    "\n",
    "new_file = open('vocabolary.tsv', 'w')\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "term_id = 1\n",
    "document_id = 1\n",
    "\n",
    "vocabolary = dict()\n",
    "diz = defaultdict(set)\n",
    "\n",
    "for book in books:\n",
    "    tokenizer = RegexpTokenizer(r\"[a-zA-Z]+\") \n",
    "    text_tokens = nodigit(tokenizer.tokenize(book.split('\\t')[6]))\n",
    "    tokens_without_sw = {word for word in text_tokens if not word in stopwords.words()}\n",
    "    for word in tokens_without_sw:\n",
    "        w = ps.stem(word.lower())\n",
    "        if w not in vocabolary:\n",
    "            vocabolary[w] = term_id\n",
    "            diz[term_id].add(document_id)\n",
    "            new_file.write(w + \"\\t\" + str(term_id) + '\\n')\n",
    "            term_id += 1\n",
    "        else:\n",
    "            diz[vocabolary[w]].add(document_id)\n",
    "    print('Finished document ' + str(document_id))\n",
    "    document_id += 1\n",
    "    \n",
    "new_file.close()\n",
    "f.close()\n",
    "\n",
    "\n",
    "with open(\"dictionary.json\", \"w\") as outfile: \n",
    "    json.dump(dict(zip(diz.keys(), map(list, diz.values()))), outfile, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_csv('index_books.tsv', header=None, sep='\\t', usecols=[0,1,7,12])\n",
    "\n",
    "ds.rename(columns={0:'index', 1:'bookTitle', 7:'plot', 12:'url'}, inplace=True)\n",
    "\n",
    "voc = dict()\n",
    "with open('vocabolary.tsv') as f:\n",
    "    for col1, col2 in csv.reader(f, delimiter='\\t'):\n",
    "        voc[col1] = col2\n",
    "        \n",
    "with open('dictionary.json') as f:\n",
    "    dt = json.load(f) # dictionary\n",
    "\n",
    "\n",
    "def query(q):\n",
    "\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    q = q.strip().split() # input from user\n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    q = [ps.stem(w).lower() for w in q]\n",
    "\n",
    "    # elaborate query\n",
    "    \n",
    "    # take term_id(s)\n",
    "    term = list()\n",
    "    for w in q:\n",
    "        try:\n",
    "            term.append(voc[w])\n",
    "        except:\n",
    "            pass\n",
    "    # matching documents\n",
    "    if len(term):\n",
    "        doc = set(dt[term[0]])\n",
    "        for i in range(1, len(term)):\n",
    "            doc = doc.intersection(dt[term[i]])\n",
    "        # take row from books\n",
    "        return ds[ds['index'].isin(list(doc))].head()\n",
    "    else:\n",
    "        return \"There aren't documents for each word of this query\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = dict()\n",
    "with open('vocabolary.tsv') as f:\n",
    "    for col1, col2 in csv.reader(f, delimiter='\\t'):\n",
    "        voc[col1] = col2\n",
    "        \n",
    "with open('dictionary.json') as f:\n",
    "    dt = json.load(f) # dictionary\n",
    "    \n",
    "ds = dict()\n",
    "with open('index_books.tsv') as f:\n",
    "    for row in csv.reader(f, delimiter='\\t'):\n",
    "        if len(row) == 13:\n",
    "            ds[row[0]] = row[7]\n",
    "\n",
    "\n",
    "result = defaultdict(list)\n",
    "term_idf = defaultdict(float)\n",
    "\n",
    "for doc_id in ds:\n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    tokenizer = RegexpTokenizer(r\"[a-zA-Z]+\") \n",
    "    text_tokens = nodigit(tokenizer.tokenize(ds[doc_id]))\n",
    "    tokens_without_sw = [ps.stem(w.lower()) for w in text_tokens if not w in stopwords.words()]\n",
    "    \n",
    "    plotLength = len(tokens_without_sw)\n",
    "    count = Counter(tokens_without_sw)\n",
    "    \n",
    "    for word in count:\n",
    "        freq = count[word]\n",
    "        try:\n",
    "            term_id = str(voc[word])\n",
    "            idf = 1.0 + math.log( float(len(ds)) / len( dt[term_id] ) )\n",
    "            tf = freq / plotLength\n",
    "            tfIdf = tf * idf\n",
    "\n",
    "            \n",
    "            heapq.heappush(result[term_id], (tfIdf, doc_id))\n",
    "            term_idf[term_id] = idf\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "inv_ind = defaultdict(list)\n",
    "for term, tup_list in result.items():\n",
    "    for tup in tup_list:\n",
    "        inv_ind[term].append( (int(tup[1]), tup[0]) )\n",
    "\n",
    "\n",
    "with open(\"inverted_index.json\", \"w\") as outfile: \n",
    "    json.dump(result, outfile, indent = 4)\n",
    "\n",
    "with open(\"term_idf.json\", \"w\") as outfile: \n",
    "    json.dump(term_idf, outfile, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('term_idf.json') as f:\n",
    "    term_idf = json.load(f)\n",
    "\n",
    "with open('inverted_index2.json') as f:\n",
    "    inverted = json.load(f)\n",
    "    \n",
    "inv_ind = defaultdict(dict)\n",
    "for term in inverted:\n",
    "    for t in inverted[term]:\n",
    "        inv_ind[term][t[0]] = t[1]\n",
    "\n",
    "dot = lambda x, y : sum(xi*yi for xi, yi in zip(x, y))\n",
    "square = lambda x : [v**2 for v in x]\n",
    "det = lambda x : math.sqrt(sum(square(x)))\n",
    "    \n",
    "def similarity(q):\n",
    "    ps = PorterStemmer()\n",
    "    # execute query\n",
    "    err = \"There aren't documents for each word of this query\"\n",
    "    q_result = query(q)\n",
    "    if not isinstance(q_result, str):\n",
    "        q = q.strip().split() # input from user\n",
    "        q = [ps.stem(w).lower() for w in q]\n",
    "        # create a list of ifidf of terms\n",
    "        term_ifidf = list()\n",
    "        tf = 1/len(q)\n",
    "        for w in q:\n",
    "            term_ifidf += [term_idf[voc[w]]*tf]\n",
    "        # create a list of ifidf of document\n",
    "        doc_ifidf = defaultdict(list)\n",
    "        for d_id in q_result['index']:\n",
    "            for w in q:\n",
    "                doc_ifidf[d_id].append(inv_ind[voc[w]][d_id])\n",
    "        #compare value and calculate similarity\n",
    "        cos_sim = list()\n",
    "        det_q = det(term_ifidf)\n",
    "        for doc in q_result['index']:\n",
    "            prod = dot(doc_ifidf[doc], term_ifidf)\n",
    "            det_doc = det(doc_ifidf[doc])\n",
    "            cos_sim += [(prod / (det_q * det_doc))]\n",
    "        q_result['similarity'] = cos_sim\n",
    "        return q_result.sort_values(by=['similarity', 'index'], ascending=False).head()\n",
    "    else:\n",
    "        return err\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_csv('index_books.tsv', header=None, sep='\\t', usecols=[0,1,2,3,4,5,6,8,9,10,11])\n",
    "\n",
    "\n",
    "ds.rename(columns={0:'index', 1:'bookTitle', 2:'bookSeries', 3:'bookAuthors', 4:'ratingValue', 5:'ratingCount', 6:'reviewCount', 8:'numberOfPages', 9:'publishingDate', 10:'characters', 11:'setting'}, inplace=True)\n",
    "\n",
    "normstring = lambda x : [i.translate(str.maketrans('', '', string.punctuation)).split() for i in x.fillna('') if len(i)>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>bookTitle</th>\n",
       "      <th>bookSeries</th>\n",
       "      <th>bookAuthors</th>\n",
       "      <th>ratingValue</th>\n",
       "      <th>ratingCount</th>\n",
       "      <th>reviewCount</th>\n",
       "      <th>numberOfPages</th>\n",
       "      <th>publishingDate</th>\n",
       "      <th>characters</th>\n",
       "      <th>setting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>(The Hunger Games #1)</td>\n",
       "      <td>Suzanne Collins</td>\n",
       "      <td>4.33</td>\n",
       "      <td>6,416,595</td>\n",
       "      <td>172,694</td>\n",
       "      <td>374</td>\n",
       "      <td>September 14th 2008</td>\n",
       "      <td>Katniss Everdeen,Peeta Mellark,Cato (Hunger Ga...</td>\n",
       "      <td>District 12, Panem,Capitol, Panem,Panem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "      <td>(Harry Potter #5)</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "      <td>4.50</td>\n",
       "      <td>2,528,825</td>\n",
       "      <td>42,823</td>\n",
       "      <td>870</td>\n",
       "      <td>September 2004</td>\n",
       "      <td>Sirius Black,Draco Malfoy,Ron Weasley,Petunia ...</td>\n",
       "      <td>Hogwarts School of Witchcraft and Wizardry,Lon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>To Kill a Mockingbird</td>\n",
       "      <td>(To Kill a Mockingbird)</td>\n",
       "      <td>Harper Lee</td>\n",
       "      <td>4.28</td>\n",
       "      <td>4,533,872</td>\n",
       "      <td>91,933</td>\n",
       "      <td>324</td>\n",
       "      <td>May 23rd 2006</td>\n",
       "      <td>Scout Finch,Atticus Finch,Jem Finch,Arthur Rad...</td>\n",
       "      <td>Maycomb, Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Pride and Prejudice</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jane Austen</td>\n",
       "      <td>4.26</td>\n",
       "      <td>3,022,330</td>\n",
       "      <td>67,957</td>\n",
       "      <td>279</td>\n",
       "      <td>October 10th 2000</td>\n",
       "      <td>Mr. Bennet,Mrs. Bennet,Jane Bennet,Elizabeth B...</td>\n",
       "      <td>United Kingdom,Derbyshire, England,England,Her...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Twilight</td>\n",
       "      <td>(The Twilight Saga #1)</td>\n",
       "      <td>Stephenie Meyer</td>\n",
       "      <td>3.60</td>\n",
       "      <td>4,996,297</td>\n",
       "      <td>105,003</td>\n",
       "      <td>501</td>\n",
       "      <td>September 6th 2006</td>\n",
       "      <td>Edward Cullen,Jacob Black,Laurent,Renee,Bella ...</td>\n",
       "      <td>Forks, Washington,Phoenix, Arizona,Washington ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>The Book Thief</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Markus Zusak</td>\n",
       "      <td>4.37</td>\n",
       "      <td>1,848,678</td>\n",
       "      <td>113,655</td>\n",
       "      <td>552</td>\n",
       "      <td>March 14th 2006</td>\n",
       "      <td>Liesel Meminger,Hans Hubermann,Rudy Steiner,Ro...</td>\n",
       "      <td>Molching,Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Animal Farm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>George Orwell</td>\n",
       "      <td>3.95</td>\n",
       "      <td>2,762,891</td>\n",
       "      <td>56,860</td>\n",
       "      <td>141</td>\n",
       "      <td>April 1996</td>\n",
       "      <td>Snowball,Napoleon,Clover,Boxer,Old Major,Murie...</td>\n",
       "      <td>England,United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>The Chronicles of Narnia</td>\n",
       "      <td>(The Chronicles of Narnia (Publication Order) ...</td>\n",
       "      <td>C.S. Lewis</td>\n",
       "      <td>4.26</td>\n",
       "      <td>522,676</td>\n",
       "      <td>10,424</td>\n",
       "      <td>767</td>\n",
       "      <td>September 16th 2002</td>\n",
       "      <td>Polly,Aslan,Lucy Pevensie,Edmund Pevensie,Eust...</td>\n",
       "      <td>London, England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>J.R.R. Tolkien 4-Book Boxed Set: The Hobbit an...</td>\n",
       "      <td>(The Lord of the Rings #0-3)</td>\n",
       "      <td>J.R.R. Tolkien</td>\n",
       "      <td>4.60</td>\n",
       "      <td>110,952</td>\n",
       "      <td>1,802</td>\n",
       "      <td>1728</td>\n",
       "      <td>September 25th 2012</td>\n",
       "      <td>Frodo Baggins,Gandalf,Bilbo Baggins,Gollum</td>\n",
       "      <td>Middle-earth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>The Fault in Our Stars</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Green</td>\n",
       "      <td>4.20</td>\n",
       "      <td>3,578,312</td>\n",
       "      <td>155,909</td>\n",
       "      <td>313</td>\n",
       "      <td>January 10th 2012</td>\n",
       "      <td>Hazel Grace Lancaster,Augustus Waters,Isaac</td>\n",
       "      <td>Indianapolis, Indiana,Amsterdam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                          bookTitle  \\\n",
       "0      1                                   The Hunger Games   \n",
       "1      2          Harry Potter and the Order of the Phoenix   \n",
       "2      3                              To Kill a Mockingbird   \n",
       "3      4                                Pride and Prejudice   \n",
       "4      5                                           Twilight   \n",
       "5      6                                     The Book Thief   \n",
       "6      7                                        Animal Farm   \n",
       "7      8                           The Chronicles of Narnia   \n",
       "8      9  J.R.R. Tolkien 4-Book Boxed Set: The Hobbit an...   \n",
       "9     10                             The Fault in Our Stars   \n",
       "\n",
       "                                          bookSeries      bookAuthors  \\\n",
       "0                              (The Hunger Games #1)  Suzanne Collins   \n",
       "1                                  (Harry Potter #5)     J.K. Rowling   \n",
       "2                            (To Kill a Mockingbird)       Harper Lee   \n",
       "3                                                NaN      Jane Austen   \n",
       "4                             (The Twilight Saga #1)  Stephenie Meyer   \n",
       "5                                                NaN     Markus Zusak   \n",
       "6                                                NaN    George Orwell   \n",
       "7  (The Chronicles of Narnia (Publication Order) ...       C.S. Lewis   \n",
       "8                       (The Lord of the Rings #0-3)   J.R.R. Tolkien   \n",
       "9                                                NaN       John Green   \n",
       "\n",
       "   ratingValue ratingCount reviewCount numberOfPages       publishingDate  \\\n",
       "0         4.33   6,416,595     172,694           374  September 14th 2008   \n",
       "1         4.50   2,528,825      42,823           870       September 2004   \n",
       "2         4.28   4,533,872      91,933           324        May 23rd 2006   \n",
       "3         4.26   3,022,330      67,957           279    October 10th 2000   \n",
       "4         3.60   4,996,297     105,003           501   September 6th 2006   \n",
       "5         4.37   1,848,678     113,655           552      March 14th 2006   \n",
       "6         3.95   2,762,891      56,860           141           April 1996   \n",
       "7         4.26     522,676      10,424           767  September 16th 2002   \n",
       "8         4.60     110,952       1,802          1728  September 25th 2012   \n",
       "9         4.20   3,578,312     155,909           313    January 10th 2012   \n",
       "\n",
       "                                          characters  \\\n",
       "0  Katniss Everdeen,Peeta Mellark,Cato (Hunger Ga...   \n",
       "1  Sirius Black,Draco Malfoy,Ron Weasley,Petunia ...   \n",
       "2  Scout Finch,Atticus Finch,Jem Finch,Arthur Rad...   \n",
       "3  Mr. Bennet,Mrs. Bennet,Jane Bennet,Elizabeth B...   \n",
       "4  Edward Cullen,Jacob Black,Laurent,Renee,Bella ...   \n",
       "5  Liesel Meminger,Hans Hubermann,Rudy Steiner,Ro...   \n",
       "6  Snowball,Napoleon,Clover,Boxer,Old Major,Murie...   \n",
       "7  Polly,Aslan,Lucy Pevensie,Edmund Pevensie,Eust...   \n",
       "8         Frodo Baggins,Gandalf,Bilbo Baggins,Gollum   \n",
       "9        Hazel Grace Lancaster,Augustus Waters,Isaac   \n",
       "\n",
       "                                             setting  \n",
       "0            District 12, Panem,Capitol, Panem,Panem  \n",
       "1  Hogwarts School of Witchcraft and Wizardry,Lon...  \n",
       "2                                   Maycomb, Alabama  \n",
       "3  United Kingdom,Derbyshire, England,England,Her...  \n",
       "4  Forks, Washington,Phoenix, Arizona,Washington ...  \n",
       "5                                   Molching,Germany  \n",
       "6                             England,United Kingdom  \n",
       "7                                    London, England  \n",
       "8                                       Middle-earth  \n",
       "9                    Indianapolis, Indiana,Amsterdam  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "normstring = lambda x : [i.translate(str.maketrans('', '', string.punctuation)).split() if len(i) > 0 else None for i in x.fillna('') ]\n",
    "\n",
    "\n",
    "def normdate(x):\n",
    "    r = list()\n",
    "    for e in x.fillna(''):\n",
    "        v = e.replace('th', ' ').replace('nd', ' ').replace('st', ' ').replace('rd', ' ')\\\n",
    "                       .translate(str.maketrans('', '', string.punctuation)).split()\n",
    "        if len(v) > 3 or len(v) == 0:\n",
    "            r.append(None)\n",
    "        else:\n",
    "            r.append(v)\n",
    "    return r\n",
    "\n",
    "\n",
    "normnumber = lambda x : [float(i) or i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "normstring = lambda x : [i.translate(str.maketrans('', '', string.punctuation)).split() if len(i) > 0 else None for i in x.fillna('') ]\n",
    "\n",
    "\n",
    "def normdate(x):\n",
    "    r = list()\n",
    "    for e in x.fillna(''):\n",
    "        v = e.replace('th', ' ').replace('nd', ' ').replace('st', ' ').replace('rd', ' ')\\\n",
    "                       .translate(str.maketrans('', '', string.punctuation)).split()\n",
    "        if len(v) > 3 or len(v) == 0:\n",
    "            r.append(None)\n",
    "        else:\n",
    "            r.append(v)\n",
    "    return r\n",
    "\n",
    "\n",
    "normnumber = lambda x : [float(i.replace(',','')) for i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'float' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-22251d52922d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnormnumber\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ratingValue'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-66-2ef510e849ba>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mnormnumber\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mii\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;34m'.'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-66-2ef510e849ba>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mnormnumber\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mii\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;34m'.'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: argument of type 'float' is not iterable"
     ]
    }
   ],
   "source": [
    "normnumber(ds['ratingValue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ciao'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'ciao'.replace('7', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ciao', 'io']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'ciao      io'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
