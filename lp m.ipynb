{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\giogi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv \n",
    "import re\n",
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import json\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"homework.txt\", \"w\")\n",
    "\n",
    "for i in range(1,301):\n",
    "    page = requests.get(\"https://www.goodreads.com/list/show/1.Best_Books_Ever?page=\" + str(i))\n",
    "    soup = BeautifulSoup(page.content, features=\"lxml\")\n",
    "    links = soup.find_all('a', itemprop='url', class_='bookTitle')\n",
    "    for link in links:\n",
    "        fullLink = link.get('href')\n",
    "        f.write('https://www.goodreads.com' + fullLink + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'D:/Storage file PC/Documenti/Università/Data Science/Anno 1/Semestre 1/Algorthmic Methods of Data Mining/Homeworks/HW3/st/'\n",
    "\n",
    "f = open(\"homework.txt\", \"r\")\n",
    "\n",
    "ff = f.readlines()\n",
    "\n",
    "for i in range(30000, len(ff)+1):\n",
    "    \n",
    "    folderName = \"folder-\" + str(i) + \"/\"\n",
    "    fileName = \"article_\" + str(i) + \".html\"\n",
    "    \n",
    "\n",
    "    url = ff[i-1]\n",
    "    \n",
    "    Path(path + folderName).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    page = requests.get(url)\n",
    "    code = str(page.text)\n",
    "\n",
    "    with open(path + folderName + fileName, \"w\", encoding=\"utf-8\") as z:\n",
    "        z.write(code)\n",
    "\n",
    "    z.close()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "personalPath = \"D:/Storage file PC/Documenti/Università/Data Science/Anno 1/Semestre 1/Algorthmic Methods of Data Mining/Homeworks/HW3/st/folder-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ['bookTitle', 'bookSeries', 'bookAuthors', 'ratingValue', \"ratingCount\",\\\n",
    "        \"reviewCount\", \"plot\", \"numberOfPages\", \"publishingDate\", \"characters\", \"setting\", \"url\"]\n",
    "\n",
    "personalPath = \"D:/Storage file PC/Documenti/Università/Data Science/Anno 1/Semestre 1/Algorthmic Methods of Data Mining/Homeworks/HW3/st/folder-\"\n",
    "\n",
    "for i in range(18001, 30001):\n",
    "    with open(personalPath + str(i) + \"/article_\" + str(i) + \".html\", 'rb') as html: \n",
    "        soup = BeautifulSoup(html,\"html.parser\")\n",
    "    #plot = soup.find('div',id='description').text.strip()  \n",
    "    try:\n",
    "        plot = soup.find('div',id='description').text.strip()\n",
    "        if detect(plot)=='en':\n",
    "            lista=[]\n",
    "\n",
    "            #title\n",
    "            try:\n",
    "                lista.append(soup.find('h1').text.strip())\n",
    "            except:\n",
    "                lista.append('')\n",
    "\n",
    "                #bookseries\n",
    "            try:\n",
    "                lista.append(soup.find('h2',id='bookSeries').text.strip())\n",
    "            except:\n",
    "                lista.append('')\n",
    "\n",
    "                #author name\n",
    "            try:\n",
    "                lista.append(soup.find('a',class_='authorName').text.strip())\n",
    "            except:\n",
    "                lista.append('')\n",
    "\n",
    "                #rating value\n",
    "            try:\n",
    "                lista.append(soup.find('span', itemprop='ratingValue').text.strip())\n",
    "            except:\n",
    "                lista.append('')\n",
    "\n",
    "                #ratingCount\n",
    "            try:\n",
    "                lista.append(soup.find_all('a',class_='gr-hyperlink',href='#other_reviews')[0]\\\n",
    "                             .text.strip().replace('\\r', '').replace('\\n', '').split()[0])\n",
    "            except:\n",
    "                lista.append('')\n",
    "\n",
    "                #reviewCount\n",
    "            try:\n",
    "                lista.append(soup.find_all('a',class_='gr-hyperlink',href='#other_reviews')[1]\\\n",
    "                             .text.strip().replace('\\r', '').replace('\\n', '').split()[0])\n",
    "            except:\n",
    "                lista.append('')\n",
    "\n",
    "                #plot\n",
    "            try:\n",
    "                if plot[-7:] == '...more':\n",
    "                    lista.append(soup.find('div',id='description').contents[3].text)\n",
    "                else:\n",
    "                    lista.append(plot)\n",
    "            except:\n",
    "                lista.append('')\n",
    "\n",
    "                #number of pages\n",
    "            try:\n",
    "                lista.append(soup.find('span', itemprop='numberOfPages').text.strip().split()[0])\n",
    "            except:\n",
    "                lista.append('')\n",
    "\n",
    "                #Publishing Date\n",
    "            try:\n",
    "                a=soup.find_all('div', class_='row')[1].text\n",
    "                match_obj = re.split('Published', re.split('by', a)[0])[1]\n",
    "                lista.append(match_obj.strip())\n",
    "            except:\n",
    "                lista.append('')\n",
    "\n",
    "                #characters\n",
    "            try:\n",
    "                l1=[]\n",
    "                for d in soup.find_all('a',href=re.compile(r'/characters/*')):\n",
    "                    l1.append(d.text)\n",
    "                    s1=\",\".join(l1)\n",
    "                lista.append(s1)\n",
    "            except:\n",
    "                lista.append('')\n",
    "\n",
    "                #setting\n",
    "            try:\n",
    "                l2=[]\n",
    "                for e in soup.find_all('a',href=re.compile(r'/places/*')):\n",
    "                    l2.append(e.text)\n",
    "                    s2=\",\".join(l2)\n",
    "                lista.append(s2)\n",
    "            except:\n",
    "                lista.append('')\n",
    "\n",
    "                #URL\n",
    "            lista.append(soup.find('link')['href'].strip())\n",
    "\n",
    "            path = personalPath + str(i) + '/article_' + str(i)+ '.tsv'\n",
    "\n",
    "            with open(path, 'w', newline='',encoding=\"utf-8\") as f_output:\n",
    "                tsv_output = csv.writer(f_output, delimiter='\\t')\n",
    "                tsv_output.writerow(data)\n",
    "                tsv_output.writerow(lista)\n",
    "                f_output.close()\n",
    "        \n",
    "        else:\n",
    "            print('This book is not in english: '+ str(i))\n",
    "            \n",
    "    except:\n",
    "        print('Missing plot for book: '+ str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "index = 1\n",
    "\n",
    "new_file = open('index_books.tsv', 'w')\n",
    "\n",
    "while i <= 30000:\n",
    "    try:\n",
    "        art_f = open(personalPath + str(i) + \"/article_\" + str(i) + \".tsv\", 'r')\n",
    "        art = art_f.readlines()[1]\n",
    "        new_file.write(str(index) + \"\\t\" + art)\n",
    "        art_f.close()\n",
    "        i += 1\n",
    "        index += 1\n",
    "    except:\n",
    "        i += 1\n",
    "\n",
    "new_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodigit = lambda wordslist : [word for word in wordslist if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"books.tsv\", 'r')\n",
    "books = f.readlines()\n",
    "\n",
    "new_file = open('vocabulary.tsv', 'w')\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "term_id = 1\n",
    "document_id = 1\n",
    "\n",
    "vocabulary = dict()\n",
    "diz = defaultdict(set)\n",
    "\n",
    "for book in books:\n",
    "    tokenizer = RegexpTokenizer(r\"[a-zA-Z]+\") \n",
    "    text_tokens = nodigit(tokenizer.tokenize(book.split('\\t')[6]))\n",
    "    tokens_without_sw = {word for word in text_tokens if not word in stopwords.words()}\n",
    "    for word in tokens_without_sw:\n",
    "        w = ps.stem(word.lower())\n",
    "        if w not in vocabulary:\n",
    "            vocabulary[w] = term_id\n",
    "            diz[term_id].add(document_id)\n",
    "            new_file.write(w + \"\\t\" + str(term_id) + '\\n')\n",
    "            term_id += 1\n",
    "        else:\n",
    "            diz[vocabulary[w]].add(document_id)\n",
    "    print('Finished document ' + str(document_id))\n",
    "    document_id += 1\n",
    "    \n",
    "new_file.close()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dictionary.json\", \"w\") as outfile: \n",
    "    json.dump(dict(zip(diz.keys(), map(list, diz.values()))), outfile, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query():\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    q = input().strip().split()         # input from user\n",
    "    q = [ps.stem(w) for w in q]\n",
    "    # import documents \n",
    "    # --> we can optimize it loading documents out of function\n",
    "    # --> but isn't very correct (formally)\n",
    "    voc = pd.read_csv(\"vocabulary.tsv\", sep='\\t', header=None, error_bad_lines=False) # vocabulary\n",
    "    with open('dictionary.json') as f:\n",
    "        dt = json.load(f) # dictionary\n",
    "    ds = pd.read_csv('index_books.tsv', header=None, sep='\\t', usecols=[0,1,7,12])\n",
    "    ds.rename(columns={0:'index', 1:'bookTitle', 7:'plot', 12:'url'}, inplace=True)\n",
    "    # elaborate query\n",
    "    # take term_id(s)\n",
    "    term = list()\n",
    "    for w in q:\n",
    "        try:\n",
    "            term.append(int(voc[voc[0]==w.lower()][1]))\n",
    "        except:\n",
    "            pass\n",
    "    # matching documents\n",
    "    if len(term):\n",
    "        doc = set(dt[str(term[0])])\n",
    "        for i in range(1, len(term)):\n",
    "            doc.intersection(dt[str(term[i])])\n",
    "        # take row from books\n",
    "        \n",
    "        return ds[ds['index'].isin(list(doc))].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fantastic search\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>bookTitle</th>\n",
       "      <th>plot</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>The Chronicles of Narnia</td>\n",
       "      <td>Journeys to the end of the world, fantastic cr...</td>\n",
       "      <td>https://www.goodreads.com/book/show/11127.The_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>146</td>\n",
       "      <td>The Wonderful Wizard of Oz</td>\n",
       "      <td>When Dorothy and her little dog Toto are caugh...</td>\n",
       "      <td>https://www.goodreads.com/book/show/236093.The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>157</td>\n",
       "      <td>A Midsummer Night's Dream</td>\n",
       "      <td>Shakespeare's intertwined love polygons begin ...</td>\n",
       "      <td>https://www.goodreads.com/book/show/1622.A_Mid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>267</td>\n",
       "      <td>James and the Giant Peach</td>\n",
       "      <td>When James accidentally drops some magic cryst...</td>\n",
       "      <td>https://www.goodreads.com/book/show/6689.James...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>308</td>\n",
       "      <td>The Mark of Athena</td>\n",
       "      <td>Annabeth is terrified. Just when she's about t...</td>\n",
       "      <td>https://www.goodreads.com/book/show/12127750-t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                   bookTitle  \\\n",
       "7        8    The Chronicles of Narnia   \n",
       "144    146  The Wonderful Wizard of Oz   \n",
       "155    157   A Midsummer Night's Dream   \n",
       "255    267   James and the Giant Peach   \n",
       "295    308          The Mark of Athena   \n",
       "\n",
       "                                                  plot  \\\n",
       "7    Journeys to the end of the world, fantastic cr...   \n",
       "144  When Dorothy and her little dog Toto are caugh...   \n",
       "155  Shakespeare's intertwined love polygons begin ...   \n",
       "255  When James accidentally drops some magic cryst...   \n",
       "295  Annabeth is terrified. Just when she's about t...   \n",
       "\n",
       "                                                   url  \n",
       "7    https://www.goodreads.com/book/show/11127.The_...  \n",
       "144  https://www.goodreads.com/book/show/236093.The...  \n",
       "155  https://www.goodreads.com/book/show/1622.A_Mid...  \n",
       "255  https://www.goodreads.com/book/show/6689.James...  \n",
       "295  https://www.goodreads.com/book/show/12127750-t...  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
